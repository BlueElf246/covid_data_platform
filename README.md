# ğŸ¦  Covid-19 Data Platform

A personal **Data Engineering** project focused on building a **Data Lake** and **Data Warehouse** for COVID-19 analytics.  
This platform enables **automated data pipelines** with **daily updates**, supporting downstream analytics and dashboarding in **Power BI**.  

Through this project, youâ€™ll learn how to:
- Design and build a **Data Lake** and **Data Warehouse** architecture  
- Apply **data modeling** (Star Schema, Medallion Architecture)  
- Implement **incremental loading** for fact and dimension tables  
- Orchestrate end-to-end pipelines using **Apache Airflow**

---

## âš™ï¸ Technologies

| Layer | Technology | Description |
|-------|-------------|-------------|
| **Data Processing** | Apache Spark (SparkSQL) | Transform and process large datasets |
| **Data Orchestration** | Apache Airflow | Automate and schedule ETL workflows |
| **Data Storage** | AWS S3 | Data Lake (Bronze â†’ Silver â†’ Gold) |
|  | AWS Redshift | Data Warehouse for analytics |
| **Data Modeling** | Star Schema | Fact and Dimension tables for BI |

---

## ğŸ”„ Data Flow

<img src="https://github.com/BlueElf246/covid_data_platform/blob/main/images/Data%20Flows.png?raw=true" alt="image" width="700" height="700">

---

## ğŸ§° ETL Architecture

<img src="https://github.com/BlueElf246/covid_data_platform/blob/main/images/ELT%20tool.png?raw=true" alt="image" width=â€œ1000â€ height=â€œ1000â€>

---

## ğŸ“ Project Structure

```bash
.
â”œâ”€â”€ common
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ db_utils.py
â”‚   â”œâ”€â”€ logger.py
â”‚   â”œâ”€â”€ s3_utils.py
â”‚   â””â”€â”€ spark_utils.py
â”œâ”€â”€ configs
â”‚   â”œâ”€â”€ config_quality_check.yaml
â”‚   â”œâ”€â”€ ingestion_config.yaml
â”‚   â”œâ”€â”€ load_config.yaml
â”‚   â””â”€â”€ transform_config.yaml
â”œâ”€â”€ dags
â”‚   â”œâ”€â”€ dags.py
â”‚   â””â”€â”€ utils_dags.py
â”œâ”€â”€ data_quality
â”‚   â”œâ”€â”€ data_quality_check.py
â”‚   â””â”€â”€ utils_check.py
â”œâ”€â”€ images
â”‚   â”œâ”€â”€ Data Flows.png
â”‚   â”œâ”€â”€ ELT tool.png
â”‚   â”œâ”€â”€ Scripts.png
â”‚   â””â”€â”€ tree.rtf
â”œâ”€â”€ ingestion
â”‚   â”œâ”€â”€ connection.py
â”‚   â”œâ”€â”€ load_landing.py
â”‚   â””â”€â”€ utils_ingest.py
â”œâ”€â”€ load
â”‚   â”œâ”€â”€ load_to_dwh.py
â”‚   â””â”€â”€ utils_load.py
â”œâ”€â”€ transformation
â”‚   â”œâ”€â”€ load_to_silver.py
â”‚   â”œâ”€â”€ transform_data.py
â”‚   â””â”€â”€ utils_transform.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ STRUCTURE.txt
```
# How To Run
This project can be run idenpendently with or without airflow. 
Refer configs_sample and rename into configs
## Clone repo
```
git clone https://github.com/BlueElf246/covid_data_platform.git
cd project
```
## Install dependencies
```
python3 -m venv venv
pip install -r requirements.txt
```
## Install Airflow
```
Please follow the instruction
install_airflow.txt
```
## Run with airflow
Run the 'load_DWH' dag from airflow
## Run without airflow
Run seperate .py files

<img src="https://github.com/BlueElf246/covid_data_platform/blob/main/images/Scripts.png?raw=true" alt="image" width=â€œ500â€ height=â€œ500â€>




