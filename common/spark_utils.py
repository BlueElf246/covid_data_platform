from pyspark.sql import SparkSession
from common.config import ConfigLoader
class SparkConnector:
    """
    Class qu·∫£n l√Ω k·∫øt n·ªëi SparkSession ‚Äî d√πng chung cho Ingestion / Transformation / Load.
    """

    def __init__(self, app_name="spark_app", s3_config=None, path=None):
        """
        :param app_name: T√™n ·ª©ng d·ª•ng Spark (hi·ªÉn th·ªã trong UI)
        :param s3_config: dict ch·ª©a th√¥ng tin AWS (t√πy ch·ªçn)
        """
        self.app_name = app_name
        if s3_config is not None:
            config = ConfigLoader(path).get(s3_config)
        self.s3_config = config or {}
        self.spark = None

    def connect(self):
        """
        Kh·ªüi t·∫°o v√† tr·∫£ v·ªÅ SparkSession, t·ª± ƒë·ªông c·∫•u h√¨nh S3 n·∫øu c√≥.
        """
        builder = (
            SparkSession.builder
            .appName(self.app_name)
            .config("spark.sql.sources.partitionOverwriteMode", "dynamic")
        )

        # N·∫øu c√≥ c·∫•u h√¨nh AWS th√¨ set cho Spark
        if self.s3_config:
            builder = builder \
                .config("spark.hadoop.fs.s3a.access.key", self.s3_config.get("AWS_ACCESS_KEY")) \
                .config("spark.hadoop.fs.s3a.secret.key", self.s3_config.get("AWS_SECRET_KEY")) \
                .config("spark.hadoop.fs.s3a.endpoint", f"s3.{self.s3_config.get('AWS_REGION', 'ap-southeast-1')}.amazonaws.com") \
                .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")

        self.spark = builder.getOrCreate()
        print(f"‚úÖ SparkSession '{self.app_name}' ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o.")
        return self.spark

    def stop(self):
        """ƒê√≥ng SparkSession khi kh√¥ng d√πng n·ªØa."""
        if self.spark:
            self.spark.stop()
            print("üõë SparkSession ƒë√£ ƒë∆∞·ª£c d·ª´ng.")
