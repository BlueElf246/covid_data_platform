import os
import yaml
import psycopg2   # ho·∫∑c cx_Oracle n·∫øu b·∫°n d√πng Oracle
from common.config import ConfigLoader
import pandas as pd
class DBHelper:
    """Class ƒë·ªÉ k·∫øt n·ªëi v√† thao t√°c v·ªõi Database."""

    def __init__(self, path, db_key="database"):
        # ƒê·ªçc config
        self.config = ConfigLoader(path).get(db_key)
        self.conn = None
        self.cursor = None

    def connect(self):
        """Kh·ªüi t·∫°o k·∫øt n·ªëi DB."""
        if not self.config:
            raise ValueError("Kh√¥ng t√¨m th·∫•y th√¥ng tin database trong config.yaml")

        try:
            self.conn = psycopg2.connect(
                host=self.config["host"],
                port=self.config.get("port", 5432),
                user=self.config["user"],
                password=self.config["password"],
                dbname=self.config["dbname"],
            )
            self.cursor = self.conn.cursor()
            print(f"‚úÖ K·∫øt n·ªëi th√†nh c√¥ng t·ªõi {self.config['type']}")
        except Exception as e:
            raise ConnectionError(f"‚ùå Kh√¥ng th·ªÉ k·∫øt n·ªëi DB: {e}")

    def execute_query(self, query, params=None, fetch=False):
        """Th·ª±c thi truy v·∫•n SQL."""
        if not self.conn:
            self.connect()

        try:
            self.cursor.execute(query, params or ())
            if fetch:
                columns = [col[0] for col in self.cursor.description]
                rows = self.cursor.fetchall()
                return pd.DataFrame(rows, columns=columns)
            else:
                self.conn.commit()
                print("‚úÖ Query executed successfully.")
        except Exception as e:
            self.conn.rollback()
            print(f"‚ö†Ô∏è L·ªói khi th·ª±c thi query: {e}")
            raise e
    def read_postgres_with_spark(self, spark, query):
        PG_HOST = self.config["host"]
        PG_PORT = self.config.get("port", 5432)
        PG_DB = self.config["dbname"]
        PG_USER = self.config["user"]
        PG_PASSWORD = self.config["password"]

        try:
            df = spark.read.jdbc(
                url=f"jdbc:postgresql://{PG_HOST}:{PG_PORT}/{PG_DB}",
                table=query,
                properties={"user": PG_USER, "password": PG_PASSWORD, "driver": "org.postgresql.Driver"}
            )
            return df
        except Exception as e:
            print("‚ùå L·ªói khi k·∫øt n·ªëi:", e)
    def close(self):
        """ƒê√≥ng k·∫øt n·ªëi DB."""
        if self.cursor:
            self.cursor.close()
        if self.conn:
            self.conn.close()
        print("üîí ƒê√£ ƒë√≥ng k·∫øt n·ªëi DB.")

    def write_to_postgres_with_spark(self, df, table_name, mode="append"):
        PG_HOST = self.config["host"]
        PG_PORT = self.config.get("port", 5432)
        PG_DB = self.config["dbname"]
        PG_USER = self.config["user"]
        PG_PASSWORD = self.config["password"]

        try:
            df.write.jdbc(
                url=f"jdbc:postgresql://{PG_HOST}:{PG_PORT}/{PG_DB}",
                table=table_name,
                mode=mode,  # "append" ho·∫∑c "overwrite"
                properties={
                    "user": PG_USER,
                    "password": PG_PASSWORD,
                    "driver": "org.postgresql.Driver"
                }
            )
            print(f"‚úÖ ƒê√£ ghi DataFrame v√†o b·∫£ng '{table_name}' (mode={mode})")
        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói khi ghi DataFrame v√†o PostgreSQL: {e}")
