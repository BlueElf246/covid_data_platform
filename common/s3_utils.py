import boto3
from io import StringIO
from pyspark.sql import SparkSession
from pyspark.sql import DataFrame
from common.config import ConfigLoader
class S3Connector:
    def __init__(self, path, s3_key):
        self.config = ConfigLoader(path).get(s3_key)
        self.aws_access_key = self.config['AWS_ACCESS_KEY']
        self.aws_secret_key = self.config['AWS_SECRET_KEY']
        self.aws_region = self.config['AWS_REGION']
        # T·∫°o S3 client
        self.s3 = boto3.client(
            's3',
            aws_access_key_id=self.aws_access_key,
            aws_secret_access_key=self.aws_secret_key,
            region_name=self.aws_region
        )
    def test_connection(self, bucket_name=None):
        """
        Ki·ªÉm tra xem c√≥ k·∫øt n·ªëi ƒë∆∞·ª£c t·ªõi S3 kh√¥ng.
        N·∫øu bucket_name ƒë∆∞·ª£c cung c·∫•p, s·∫Ω th·ª≠ list object trong bucket ƒë√≥.
        """
        try:
            # G·ªçi API ƒë∆°n gi·∫£n ƒë·ªÉ ki·ªÉm tra credentials
            self.s3.list_buckets()
            print("‚úÖ AWS credentials h·ª£p l·ªá.")

            if bucket_name:
                # Ki·ªÉm tra quy·ªÅn truy c·∫≠p bucket c·ª• th·ªÉ
                response = self.s3.list_objects_v2(Bucket=bucket_name, MaxKeys=1)
                if "Contents" in response:
                    print(f"‚úÖ K·∫øt n·ªëi th√†nh c√¥ng t·ªõi bucket: {bucket_name}")
                else:
                    print(f"‚ÑπÔ∏è  Bucket {bucket_name} tr·ªëng nh∆∞ng c√≥ th·ªÉ truy c·∫≠p ƒë∆∞·ª£c.")
            return True

        except self.s3.exceptions.ClientError as e:
            print(f"‚ùå L·ªói khi k·∫øt n·ªëi t·ªõi S3: {e}")
            return False
        except Exception as e:
            print(f"‚ùå L·ªói kh√¥ng x√°c ƒë·ªãnh: {e}")
            return False


    def upload_dataframe(self, df, bucket, key):
        """Upload pandas DataFrame l√™n S3"""
        csv_buffer = StringIO()
        df.to_csv(csv_buffer, index=False)
        self.s3.put_object(Bucket=bucket, Key=key, Body=csv_buffer.getvalue())
        print(f"‚úÖ Uploaded {key} to bucket {bucket}")

    def check_exists(self, bucket, folder):
        """Ki·ªÉm tra file c√≥ t·ªìn t·∫°i tr√™n S3 hay kh√¥ng"""
        try:
            if self.s3.list_objects_v2(Bucket=bucket, Prefix=folder, MaxKeys=1)['KeyCount'] >= 1:
                return True
            else:
                return False
        except self.s3.exceptions.ClientError as e:
            print(e)
            if e.response['Error']['Code'] == "404":
                return False
            else:
                raise e

    def download_file(self, bucket, key, local_path):
        """T·∫£i file t·ª´ S3 v·ªÅ"""
        self.s3.download_file(bucket, key, local_path)
        print(f"‚¨áÔ∏è  Downloaded {key} ‚Üí {local_path}")

    def read_parquet_spark(self, spark: SparkSession, bucket: str, key: str) -> DataFrame:
        """ƒê·ªçc parquet t·ª´ S3 b·∫±ng Spark"""
        s3_path = f"s3a://{bucket}/{key}"

        # C·∫•u h√¨nh credential cho Spark
        hadoop_conf = spark._jsc.hadoopConfiguration()
        hadoop_conf.set("fs.s3a.access.key", self.aws_access_key)
        hadoop_conf.set("fs.s3a.secret.key", self.aws_secret_key)
        hadoop_conf.set("fs.s3a.endpoint", f"s3.{self.aws_region}.amazonaws.com")

        print(f"üìñ Reading Parquet from {s3_path}")
        return spark.read.parquet(s3_path)

    def write_parquet_spark(self, df: DataFrame, bucket: str, key: str, mode="overwrite", partition_col=None):
        """Ghi DataFrame Spark xu·ªëng S3"""
        s3_path = f"s3a://{bucket}/{key}"
        hadoop_conf = df.sparkSession._jsc.hadoopConfiguration()
        hadoop_conf.set("fs.s3a.access.key", self.aws_access_key)
        hadoop_conf.set("fs.s3a.secret.key", self.aws_secret_key)
        hadoop_conf.set("fs.s3a.endpoint", f"s3.{self.aws_region}.amazonaws.com")

        writer = df.write.mode(mode)
        if partition_col:
            writer = writer.partitionBy(partition_col)

        writer.parquet(s3_path)
        print(f"‚úÖ Wrote Parquet to {s3_path} (mode={mode})")

    def query_data_spark(self, spark, table_list, query):
        """
            spark (SparkSession): The Spark session to use for executing the query.
            table_list (list of tuples): List of (name, DataFrame) pairs to register as temporary views.
            query (str): The SQL query to execute.

            Returns:
            DataFrame: The result of the SQL query as a Spark DataFrame.
        """
        for name, df in table_list:
            df.createOrReplaceTempView(name)
        result = spark.sql(query)
        return result